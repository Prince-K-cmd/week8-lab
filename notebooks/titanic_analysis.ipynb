{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic - Machine Learning from Disaster\n",
    "\n",
    "## Overview\n",
    "This notebook contains a comprehensive analysis of the Titanic dataset, implementing various machine learning techniques to predict passenger survival. We'll follow a structured approach to solve this problem, going through data exploration, preprocessing, feature engineering, model selection, and optimization.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Exploration and Visualization](#1.-Data-Exploration-and-Visualization)\n",
    "2. [Data Cleaning and Preprocessing](#2.-Data-Cleaning-and-Preprocessing)\n",
    "3. [Feature Engineering](#3.-Feature-Engineering)\n",
    "4. [Model Selection and Training](#4.-Model-Selection-and-Training)\n",
    "5. [Model Optimization](#5.-Model-Optimization)\n",
    "6. [Testing and Submission](#6.-Testing-and-Submission)\n",
    "\n",
    "## Setup\n",
    "First, let's import all necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Data manipulation and analysis\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Custom modules\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Add src directory to Python path\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Custom modules\n",
    "from src.data_processing import (\n",
    "    load_data,\n",
    "    handle_missing_values,\n",
    "    encode_categorical_features,\n",
    "    create_features,\n",
    "    scale_features,\n",
    "    prepare_data\n",
    ")\n",
    "from src.visualization import (\n",
    "    set_plotting_style,\n",
    "    plot_survival_by_feature,\n",
    "    plot_age_distribution,\n",
    "    plot_correlation_matrix,\n",
    "    plot_feature_importance,\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curve,\n",
    "    plot_model_comparison,\n",
    "    create_analysis_plots\n",
    ")\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "set_plotting_style()\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Exploration and Visualization\n",
    "\n",
    "In this section, we'll:\n",
    "1. Load the dataset\n",
    "2. Analyze basic statistics\n",
    "3. Visualize relationships between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_df, test_df = load_data('../data/raw/train.csv', '../data/raw/test.csv')\n",
    "\n",
    "print(\"Training set shape:\", train_df.shape)\n",
    "print(\"Test set shape:\", test_df.shape)\n",
    "\n",
    "# Display first few rows\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive analysis plots\n",
    "create_analysis_plots(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning and Preprocessing\n",
    "\n",
    "Now we'll prepare our data for modeling by:\n",
    "1. Handling missing values\n",
    "2. Encoding categorical variables\n",
    "3. Scaling numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data using our custom functions\n",
    "X_train, X_val, y_train, y_val, test_processed, scaler = prepare_data(train_df, test_df)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape)\n",
    "print(\"Test set shape:\", test_processed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering\n",
    "\n",
    "Our prepare_data function has already created several new features:\n",
    "1. FamilySize: Combined SibSp and Parch\n",
    "2. IsAlone: Binary indicator for solo travelers\n",
    "3. FarePerPerson: Fare divided by family size\n",
    "4. AgeGroup: Categorized age into meaningful groups\n",
    "\n",
    "Let's analyze the importance of these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest to get feature importance\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plot_feature_importance(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Selection and Training\n",
    "\n",
    "We'll train and evaluate three different models:\n",
    "1. Logistic Regression\n",
    "2. Random Forest\n",
    "3. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_val, y_train, y_val):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    }\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    results[name] = evaluate_model(model, X_train, X_val, y_train, y_val)\n",
    "\n",
    "# Create and display comparison plot\n",
    "results_df = pd.DataFrame(results).round(3)\n",
    "plot_model_comparison(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Optimization\n",
    "\n",
    "We'll perform hyperparameter tuning for each model using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 5, 10, 15],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform grid search for each model\n",
    "optimized_models = {}\n",
    "optimized_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nOptimizing {name}...\")\n",
    "    grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    optimized_models[name] = grid_search.best_estimator_\n",
    "    optimized_results[name] = evaluate_model(grid_search.best_estimator_,\n",
    "                                            X_train,\n",
    "                                            X_val,\n",
    "                                            y_train,\n",
    "                                            y_val)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Create and display optimized model comparison plot\n",
    "optimized_results_df = pd.DataFrame(optimized_results).round(3)\n",
    "plot_model_comparison(optimized_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Testing and Submission\n",
    "\n",
    "We'll use the best performing model to make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model based on validation accuracy\n",
    "best_model_name = max(optimized_results, key=lambda k: optimized_results[k]['Accuracy'])\n",
    "best_model = optimized_models[best_model_name]\n",
    "\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = best_model.predict(test_processed)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Survived': test_predictions\n",
    "})\n",
    "\n",
    "# Save predictions\n",
    "submission.to_csv('../Prince_submission.csv', index=False)\n",
    "print(\"\\nSubmission file has been created!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
