{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic - Machine Learning from Disaster\n",
    "\n",
    "## Overview\n",
    "This notebook contains a comprehensive analysis of the Titanic dataset, implementing various machine learning techniques to predict passenger survival. We'll follow a structured approach to solve this problem, going through data exploration, preprocessing, feature engineering, model selection, and optimization.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Exploration and Visualization](#1.-Data-Exploration-and-Visualization)\n",
    "2. [Data Cleaning and Preprocessing](#2.-Data-Cleaning-and-Preprocessing)\n",
    "3. [Feature Engineering](#3.-Feature-Engineering)\n",
    "4. [Model Selection and Training](#4.-Model-Selection-and-Training)\n",
    "5. [Model Optimization](#5.-Model-Optimization)\n",
    "6. [Testing and Submission](#6.-Testing-and-Submission)\n",
    "\n",
    "## Setup\n",
    "First, let's import all necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Exploration and Visualization\n",
    "\n",
    "In this section, we'll:\n",
    "1. Load the dataset\n",
    "2. Analyze basic statistics\n",
    "3. Visualize relationships between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the datasets\n",
    "train_df = pd.read_csv('../data/raw/train.csv')\n",
    "test_df = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "print(\"Training set shape:\", train_df.shape)\n",
    "print(\"Test set shape:\", test_df.shape)\n",
    "\n",
    "# Display first few rows\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "train_df.info()\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check missing values\n",
    "missing_values = pd.DataFrame({\n",
    "    'Missing Values': train_df.isnull().sum(),\n",
    "    'Percentage': (train_df.isnull().sum() / len(train_df)) * 100\n",
    "})\n",
    "print(\"Missing Values Analysis:\")\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Key Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Survival rate by gender\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Sex', y='Survived', data=train_df)\n",
    "plt.title('Survival Rate by Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Survival rate by passenger class\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Pclass', y='Survived', data=train_df)\n",
    "plt.title('Survival Rate by Passenger Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Age distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=train_df, x='Age', hue='Survived', multiple=\"stack\", bins=30)\n",
    "plt.title('Age Distribution by Survival Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Correlation matrix\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = train_df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning and Preprocessing\n",
    "\n",
    "In this section, we'll:\n",
    "1. Handle missing values\n",
    "2. Encode categorical variables\n",
    "3. Scale numerical features\n",
    "4. Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def preprocess_data(df, is_training=True):\n",
    "    # Create a copy of the dataframe\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    # Age: Fill with median age\n",
    "    df_processed['Age'].fillna(df_processed['Age'].median(), inplace=True)\n",
    "    \n",
    "    # Embarked: Fill with most common value\n",
    "    df_processed['Embarked'].fillna(df_processed['Embarked'].mode()[0], inplace=True)\n",
    "    \n",
    "    # Fare: Fill with median fare\n",
    "    df_processed['Fare'].fillna(df_processed['Fare'].median(), inplace=True)\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    # Sex: Convert to numeric\n",
    "    df_processed['Sex'] = df_processed['Sex'].map({'male': 0, 'female': 1})\n",
    "    \n",
    "    # Embarked: Convert to numeric\n",
    "    embarked_mapping = {'S': 0, 'C': 1, 'Q': 2}\n",
    "    df_processed['Embarked'] = df_processed['Embarked'].map(embarked_mapping)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = ['Name', 'Ticket', 'Cabin', 'PassengerId']\n",
    "    df_processed.drop(columns=columns_to_drop, inplace=True)\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Preprocess training and test data\n",
    "train_processed = preprocess_data(train_df)\n",
    "test_processed = preprocess_data(test_df, is_training=False)\n",
    "\n",
    "# Split features and target for training data\n",
    "X = train_processed.drop('Survived', axis=1)\n",
    "y = train_processed['Survived']\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features = ['Age', 'Fare']\n",
    "X[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "test_processed[numerical_features] = scaler.transform(test_processed[numerical_features])\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering\n",
    "\n",
    "In this section, we'll create new features and analyze their importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def engineer_features(df):\n",
    "    df_engineered = df.copy()\n",
    "    \n",
    "    # Create family size feature\n",
    "    df_engineered['FamilySize'] = df_engineered['SibSp'] + df_engineered['Parch'] + 1\n",
    "    \n",
    "    # Create is_alone feature\n",
    "    df_engineered['IsAlone'] = (df_engineered['FamilySize'] == 1).astype(int)\n",
    "    \n",
    "    # Create fare per person feature\n",
    "    df_engineered['FarePerPerson'] = df_engineered['Fare'] / df_engineered['FamilySize']\n",
    "    \n",
    "    # Create age groups\n",
    "    df_engineered['AgeGroup'] = pd.cut(df_engineered['Age'], \n",
    "                                       bins=[0, 12, 18, 35, 50, 100],\n",
    "                                       labels=[0, 1, 2, 3, 4])\n",
    "    \n",
    "    return df_engineered\n",
    "\n",
    "# Apply feature engineering\n",
    "X_train_engineered = engineer_features(X_train)\n",
    "X_val_engineered = engineer_features(X_val)\n",
    "test_engineered = engineer_features(test_processed)\n",
    "\n",
    "# Scale new numerical features\n",
    "new_numerical_features = ['FamilySize', 'FarePerPerson']\n",
    "scaler_new = StandardScaler()\n",
    "X_train_engineered[new_numerical_features] = scaler_new.fit_transform(X_train_engineered[new_numerical_features])\n",
    "X_val_engineered[new_numerical_features] = scaler_new.transform(X_val_engineered[new_numerical_features])\n",
    "test_engineered[new_numerical_features] = scaler_new.transform(test_engineered[new_numerical_features])\n",
    "\n",
    "# Feature importance analysis using Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_engineered, y_train)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_engineered.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "plt.title('Feature Importance Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Selection and Training\n",
    "\n",
    "We'll train and evaluate three different models:\n",
    "1. Logistic Regression\n",
    "2. Random Forest\n",
    "3. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_model(model, X_train, X_val, y_train, y_val):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    }\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    results[name] = evaluate_model(model, X_train_engineered, X_val_engineered, y_train, y_val)\n",
    "    \n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).round(3)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Optimization\n",
    "\n",
    "We'll perform hyperparameter tuning for each model using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 5, 10, 15],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform grid search for each model\n",
    "optimized_models = {}\n",
    "optimized_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nOptimizing {name}...\")\n",
    "    grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train_engineered, y_train)\n",
    "    \n",
    "    optimized_models[name] = grid_search.best_estimator_\n",
    "    optimized_results[name] = evaluate_model(grid_search.best_estimator_,\n",
    "                                            X_train_engineered,\n",
    "                                            X_val_engineered,\n",
    "                                            y_train,\n",
    "                                            y_val)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Display optimized results\n",
    "optimized_results_df = pd.DataFrame(optimized_results).round(3)\n",
    "print(\"\\nOptimized Model Comparison:\")\n",
    "print(optimized_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Testing and Submission\n",
    "\n",
    "We'll use the best performing model to make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Find the best model based on validation accuracy\n",
    "best_model_name = max(optimized_results, key=lambda k: optimized_results[k]['Accuracy'])\n",
    "best_model = optimized_models[best_model_name]\n",
    "\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = best_model.predict(test_engineered)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Survived': test_predictions\n",
    "})\n",
    "\n",
    "# Save predictions\n",
    "submission.to_csv('../Prince_submission.csv', index=False)\n",
    "print(\"\\nSubmission file has been created!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
